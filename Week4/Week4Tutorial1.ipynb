{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week4Tutorial1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPgnbTOWsctUMCHirQb4RN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebatty/MathToolsforNeuroscience/blob/master/Week4/Week4Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9l0lyqLvkW7"
      },
      "source": [
        "# Week 4: Linear Algebra IV\n",
        "\n",
        "# Tutorial 1\n",
        "\n",
        "# [insert your name]\n",
        "\n",
        "**Important reminders**: Before starting, click \"File -> Save a copy in Drive\". Produce a pdf for submission by \"File -> Print\" and then choose \"Save to PDF\".\n",
        "\n",
        "To complete this tutorial, you should have watched Videos 4.1, 4.2, 4.3, and 4.4.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7on9Hsec0Y9"
      },
      "source": [
        "**Credits**: Video 4.3 is the Week 1 Day 5 Intro Video from NMA (Neuromatch Academy) (https://github.com/NeuromatchAcademy/course-content). Exercise 2/3 in this tutorial are modified from content in NMA W1D5 tutorials.\n",
        "\n",
        "We are again using code for visualizing linear transformations from https://openedx.seas.gwu.edu/courses/course-v1:GW+EngComp4+2019/about. In particular, we are using their `plot_linear_transformation` and `plot_linear_transformations` functions.\n",
        "\n",
        "The interactive demo in Exercise 2A is based on matlab code from: https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv9HSBNPyLV9",
        "cellView": "form"
      },
      "source": [
        "# @markdown Imports\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets  # interactive display\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIdPVYl9TzmK",
        "cellView": "form"
      },
      "source": [
        "# @markdown Plotting functions\n",
        "import numpy\n",
        "from numpy.linalg import inv, eig\n",
        "from math import ceil\n",
        "from matplotlib import pyplot, ticker, get_backend, rc\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "classic = 'k'\n",
        "\n",
        "_int_backends = ['GTK3Agg', 'GTK3Cairo', 'MacOSX', 'nbAgg',\n",
        "                 'Qt4Agg', 'Qt4Cairo', 'Qt5Agg', 'Qt5Cairo',\n",
        "                 'TkAgg', 'TkCairo', 'WebAgg', 'WX', 'WXAgg', 'WXCairo']\n",
        "_backend = get_backend()   # get current backend name\n",
        "\n",
        "# shrink figsize and fontsize when using %matplotlib notebook\n",
        "if _backend in _int_backends:\n",
        "    fontsize = 4\n",
        "    fig_scale = 0.75\n",
        "else:\n",
        "    fontsize = 5\n",
        "    fig_scale = 1\n",
        "\n",
        "grey = '#808080'\n",
        "gold = '#cab18c'   # x-axis grid\n",
        "lightblue = '#0096d6'  # y-axis grid\n",
        "green = '#008367'  # x-axis basis vector\n",
        "red = '#E31937'    # y-axis basis vector\n",
        "darkblue = '#004065'\n",
        "\n",
        "pink, yellow, orange, purple, brown = '#ef7b9d', '#fbd349', '#ffa500', '#a35cff', '#731d1d'\n",
        "\n",
        "quiver_params = {'angles': 'xy',\n",
        "                 'scale_units': 'xy',\n",
        "                 'scale': 1,\n",
        "                 'width': 0.012}\n",
        "\n",
        "grid_params = {'linewidth': 0.5,\n",
        "               'alpha': 0.8}\n",
        "def plot_sample_images(X):\n",
        "  \"\"\"\n",
        "  Plots 9 images from the data.\n",
        "\n",
        "  Args:\n",
        "     X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                 different random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "  im_size = int(np.sqrt(X.shape[1]))\n",
        "  fig, ax = plt.subplots()\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(X[k, :], (im_size, im_size)),\n",
        "                 extent=[(k1 + 1) * im_size, k1 * im_size, (k2+1) * im_size, k2 * im_size],\n",
        "                 vmin=0, vmax=255, cmap='gray')\n",
        "  plt.xlim((3 * im_size, 0))\n",
        "  plt.ylim((3 * im_size, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  plt.clim([0, 250])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_variance_explained(variance_explained):\n",
        "  \"\"\"\n",
        "  Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "    variance_explained (numpy array of floats) : Vector of variance explained\n",
        "                                                 for each PC\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.plot(np.arange(1, len(variance_explained) + 1), variance_explained,\n",
        "           '--k')\n",
        "  plt.xlabel('Number of components')\n",
        "  plt.ylabel('Variance explained')\n",
        "  plt.show()\n",
        "\n",
        "def plot_reconstructions(X, X_reconstructed):\n",
        "  \"\"\"\n",
        "  Plots 9 images in the dataset side-by-side with the reconstructed\n",
        "  images.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats)               : Data matrix each column\n",
        "                                              corresponds to a different\n",
        "                                              random variable\n",
        "    X_reconstructed (numpy array of floats) : Data matrix each column\n",
        "                                              corresponds to a different\n",
        "                                              random variable\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  im_size = int(np.sqrt(X.shape[1]))\n",
        "\n",
        "  plt.figure()\n",
        "  ax = plt.subplot(121)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(X[k, :], (im_size, im_size)),\n",
        "                 extent=[(k1 + 1) * im_size, k1 * im_size, (k2 + 1) * im_size, k2 * im_size],\n",
        "                 vmin=0, vmax=255, cmap='gray')\n",
        "  plt.xlim((3 * im_size, 0))\n",
        "  plt.ylim((3 * im_size, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.title('Data')\n",
        "  plt.clim([0, 250])\n",
        "  ax = plt.subplot(122)\n",
        "  k = 0\n",
        "  for k1 in range(3):\n",
        "    for k2 in range(3):\n",
        "      k = k + 1\n",
        "      plt.imshow(np.reshape(np.real(X_reconstructed[k, :]), (im_size, im_size)),\n",
        "                 extent=[(k1 + 1) * im_size, k1 * im_size, (k2 + 1) * im_size, k2 * im_size],\n",
        "                 vmin=0, vmax=255, cmap='gray')\n",
        "  plt.xlim((3 * im_size, 0))\n",
        "  plt.ylim((3 * im_size, 0))\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.clim([0, 250])\n",
        "  plt.title('Reconstructed')\n",
        "  plt.tight_layout()\n",
        "\n",
        "def plot_principal_components(weights):\n",
        "  \"\"\"\n",
        "  Visualize PCA basis vector weights. Red = positive weights,\n",
        "  blue = negative weights, white = zero weight.\n",
        "\n",
        "  Args:\n",
        "     weights (numpy array of floats) : PCA basis vector\n",
        "\n",
        "  Returns:\n",
        "     Nothing.\n",
        "  \"\"\"\n",
        "  im_size = int(np.sqrt(X.shape[1]))\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  cmap = plt.cm.get_cmap('seismic')\n",
        "  plt.imshow(np.real(np.reshape(weights, (im_size, im_size))), cmap=cmap)\n",
        "  plt.tick_params(axis='both', which='both', bottom=False, top=False,\n",
        "                  labelbottom=False)\n",
        "  plt.clim(-.15, .15)\n",
        "  plt.colorbar(ticks=[-.15, -.1, -.05, 0, .05, .1, .15])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_pca_transformation(data, transformed_data):\n",
        "\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2)\n",
        "    axes[0].scatter(data[:,0], data[:, 1], s=1, c='#63BA79');\n",
        "    for j in range(2):\n",
        "      axes[j].spines['right'].set_visible(False)\n",
        "      axes[j].spines['top'].set_visible(False)\n",
        "\n",
        "    orig_correlation = round(np.corrcoef(data[:, 0], data[:, 1])[0, 1], 2)\n",
        "    axes[0].set(title='Data in original coordinates \\n Correlation = ' + str(orig_correlation), xlabel='Neuron 1 activity', ylabel='Neuron 2 activity', xlim=[-5, 15], ylim=[-5, 15]);\n",
        "\n",
        "    axes[1].scatter(transformed_data[:,0], transformed_data[:, 1], s=1, c='#63BA79');\n",
        "    pca_correlation = round(np.corrcoef(transformed_data[:, 0], transformed_data[:, 1])[0, 1], 2)\n",
        "    axes[1].set(title='Data in PC coordinates  \\n Correlation = ' + str(pca_correlation), xlabel='PC 1', ylabel='PC 2');\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "def plot_data_and_PCs(X, W):\n",
        "  \"\"\"\n",
        "  Plots bivariate data as well as new basis vectors.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats) : Data matrix each column corresponds to a\n",
        "                                different random variable\n",
        "    W (numpy array of floats) : Square matrix representing new orthonormal\n",
        "                                basis each column represents a basis vector\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "  \"\"\"\n",
        "\n",
        "  plt.figure()\n",
        "  plt.scatter(X[:, 0], X[:, 1], s=1, color='#63BA79')\n",
        "  plt.axis('equal')\n",
        "  plt.xlabel('Neuron 1 activity')\n",
        "  plt.ylabel('Neuron 2 activity')\n",
        "  colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "  plt.plot([0, W[0, 0]], [0, W[1, 0]], color=colors[4], linewidth=1,\n",
        "           label='Component 1')\n",
        "  plt.plot([0, W[0, 1]], [0, W[1, 1]], color=colors[3], linewidth=1,\n",
        "           label='Component 2')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def plot_vector(vectors, tails=None):\n",
        "    ''' Draw 2d vectors based on the values of the vectors and the position of their tails.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    vectors : list.\n",
        "        List of 2-element array-like structures, each represents a 2d vector.\n",
        "    \n",
        "    tails : list, optional.\n",
        "        List of 2-element array-like structures, each represents the coordinates of the tail\n",
        "        of the corresponding vector in vectors. If None (default), all tails are set at the\n",
        "        origin (0,0). If len(tails) is 1, all tails are set at the same position. Otherwise,\n",
        "        vectors and tails must have the same length.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> v = [(1, 3), (3, 3), (4, 6)]\n",
        "    >>> plot_vector(v)      # draw 3 vectors with their tails at origin\n",
        "    >>> t = [numpy.array((2, 2))]\n",
        "    >>> plot_vector(v, t)   # draw 3 vectors with their tails at (2,2)\n",
        "    >>> t = [[3, 2], [-1, -2], [3, 5]]\n",
        "    >>> plot_vector(v, t)   # draw 3 vectors with 3 different tails\n",
        "\n",
        "    '''   \n",
        "    vectors = numpy.array(vectors)\n",
        "    assert vectors.shape[1] == 2, \"Each vector should have 2 elements.\"  \n",
        "    if tails is not None:\n",
        "        tails = numpy.array(tails)\n",
        "        assert tails.shape[1] == 2, \"Each tail should have 2 elements.\"\n",
        "    else:\n",
        "        tails = numpy.zeros_like(vectors)\n",
        "    \n",
        "    # tile vectors or tails array if needed\n",
        "    nvectors = vectors.shape[0]\n",
        "    ntails = tails.shape[0]\n",
        "    if nvectors == 1 and ntails > 1:\n",
        "        vectors = numpy.tile(vectors, (ntails, 1))\n",
        "    elif ntails == 1 and nvectors > 1:\n",
        "        tails = numpy.tile(tails, (nvectors, 1))\n",
        "    else:\n",
        "        assert tails.shape == vectors.shape, \"vectors and tail must have a same shape\"\n",
        "\n",
        "    # calculate xlimit & ylimit\n",
        "    heads = tails + vectors\n",
        "    limit = numpy.max(numpy.abs(numpy.hstack((tails, heads))))\n",
        "    limit = numpy.ceil(limit * 1.2)   # add some margins\n",
        "    \n",
        "    figsize = numpy.array([2,2]) * fig_scale\n",
        "    figure, axis = pyplot.subplots(figsize=figsize)\n",
        "    axis.quiver(tails[:,0], tails[:,1], vectors[:,0], vectors[:,1], color=darkblue, \n",
        "                  angles='xy', scale_units='xy', scale=1)\n",
        "    axis.set_xlim([-limit, limit])\n",
        "    axis.set_ylim([-limit, limit])\n",
        "    axis.set_aspect('equal')\n",
        "\n",
        "    # if xticks and yticks of grid do not match, choose the finer one\n",
        "    xticks = axis.get_xticks()\n",
        "    yticks = axis.get_yticks()\n",
        "    dx = xticks[1] - xticks[0]\n",
        "    dy = yticks[1] - yticks[0]\n",
        "    base = max(int(min(dx, dy)), 1)   # grid interval is always an integer\n",
        "    loc = ticker.MultipleLocator(base=base)\n",
        "    axis.xaxis.set_major_locator(loc)\n",
        "    axis.yaxis.set_major_locator(loc)\n",
        "    axis.grid(True, **grid_params)\n",
        "    \n",
        "    # show x-y axis in the center, hide frames\n",
        "    axis.spines['left'].set_position('center')\n",
        "    axis.spines['bottom'].set_position('center')\n",
        "    axis.spines['right'].set_color('none')\n",
        "    axis.spines['top'].set_color('none')\n",
        "\n",
        "def plot_transformation_helper(axis, matrix, *vectors, unit_vector=True, unit_circle=False, title=None):\n",
        "    \"\"\" A helper function to plot the linear transformation defined by a 2x2 matrix.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    axis : class matplotlib.axes.Axes.\n",
        "        The axes to plot on.\n",
        "\n",
        "    matrix : class numpy.ndarray.\n",
        "        The 2x2 matrix to visualize.\n",
        "\n",
        "    *vectors : class numpy.ndarray.\n",
        "        The vector(s) to plot along with the linear transformation. Each array denotes a vector's\n",
        "        coordinates before the transformation and must have a shape of (2,). Accept any number of vectors. \n",
        "    \n",
        "    unit_vector : bool, optional.\n",
        "        Whether to plot unit vectors of the standard basis, default to True.\n",
        "    \n",
        "    unit_circle: bool, optional.\n",
        "        Whether to plot unit circle, default to False.\n",
        "    \n",
        "    title: str, optional.\n",
        "        Title of the plot.\n",
        "\n",
        "    \"\"\"\n",
        "    assert matrix.shape == (2,2), \"the input matrix must have a shape of (2,2)\"\n",
        "    grid_range = 20\n",
        "    x = numpy.arange(-grid_range, grid_range+1)\n",
        "    X_, Y_ = numpy.meshgrid(x,x)\n",
        "    I = matrix[:,0]\n",
        "    J = matrix[:,1]\n",
        "    X = I[0]*X_ + J[0]*Y_\n",
        "    Y = I[1]*X_ + J[1]*Y_\n",
        "    origin = numpy.zeros(1)\n",
        "        \n",
        "    # draw grid lines\n",
        "    for i in range(x.size):\n",
        "        axis.plot(X[i,:], Y[i,:], c=gold, **grid_params)\n",
        "        axis.plot(X[:,i], Y[:,i], c=lightblue, **grid_params)\n",
        "    \n",
        "    # draw (transformed) unit vectors\n",
        "    if unit_vector:\n",
        "        axis.quiver(origin, origin, [I[0]], [I[1]], color=green, **quiver_params)\n",
        "        axis.quiver(origin, origin, [J[0]], [J[1]], color=red, **quiver_params)\n",
        "\n",
        "    # draw optional vectors\n",
        "    color_cycle = cycle([pink, darkblue, orange, purple, brown])\n",
        "    if vectors:\n",
        "        for vector in vectors:\n",
        "            color = next(color_cycle)\n",
        "            vector_ = matrix @ vector.reshape(-1,1)\n",
        "            axis.quiver(origin, origin, [vector_[0]], [vector_[1]], color=color, **quiver_params)\n",
        "\n",
        "    # draw optional unit circle\n",
        "    if unit_circle:\n",
        "        alpha =  numpy.linspace(0, 2*numpy.pi, 41)\n",
        "        circle = numpy.vstack((numpy.cos(alpha), numpy.sin(alpha)))\n",
        "        circle_trans = matrix @ circle\n",
        "        axis.plot(circle_trans[0], circle_trans[1], color=red, lw=0.8)\n",
        "\n",
        "    # hide frames, set xlimit & ylimit, set title\n",
        "    limit = 4\n",
        "    axis.spines['left'].set_position('center')\n",
        "    axis.spines['bottom'].set_position('center')\n",
        "    axis.spines['left'].set_linewidth(0.3)\n",
        "    axis.spines['bottom'].set_linewidth(0.3)\n",
        "    axis.spines['right'].set_color('none')\n",
        "    axis.spines['top'].set_color('none')\n",
        "    axis.set_xlim([-limit, limit])\n",
        "    axis.set_ylim([-limit, limit])\n",
        "    if title is not None:\n",
        "        axis.set_title(title)\n",
        "\n",
        "def plot_linear_transformation(matrix, *vectors, unit_vector=True, unit_circle=False):\n",
        "    \"\"\" Plot the linear transformation defined by a 2x2 matrix using the helper\n",
        "    function plot_transformation_helper(). It will create 2 subplots to visualize some\n",
        "    vectors before and after the transformation.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    matrix : class numpy.ndarray.\n",
        "        The 2x2 matrix to visualize.\n",
        "\n",
        "    *vectors : class numpy.ndarray.\n",
        "        The vector(s) to plot along with the linear transformation. Each array denotes a vector's\n",
        "        coordinates before the transformation and must have a shape of (2,). Accept any number of vectors.\n",
        "    \n",
        "    unit_vector : bool, optional.\n",
        "        Whether to plot unit vectors of the standard basis, default to True.\n",
        "    \n",
        "    unit_circle: bool, optional.\n",
        "        Whether to plot unit circle, default to False.\n",
        "    \n",
        "    \"\"\"\n",
        "    with plt.rc_context({\"figure.dpi\": 200, 'font.family':'serif', 'axes.axisbelow':True, 'font.size':fontsize, \"axes.titlesize\":5, \"lines.linewidth\":1}):\n",
        "      figsize = numpy.array([4,2]) * fig_scale\n",
        "      figure, (axis1, axis2) = pyplot.subplots(1, 2, figsize=figsize)\n",
        "      plot_transformation_helper(axis1, numpy.identity(2), *vectors, unit_vector=unit_vector, unit_circle=unit_circle, title='Before transformation')\n",
        "      plot_transformation_helper(axis2, matrix, *vectors, unit_vector=unit_vector, unit_circle=unit_circle, title='After transformation')\n",
        "\n",
        "def plot_linear_transformations(*matrices, unit_vector=True, unit_circle=False):\n",
        "    \"\"\" Plot the linear transformation defined by a sequence of n 2x2 matrices using the helper\n",
        "    function plot_transformation_helper(). It will create n+1 subplots to visualize some\n",
        "    vectors before and after each transformation.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    *matrices : class numpy.ndarray.\n",
        "        The 2x2 matrices to visualize. Accept any number of matrices.\n",
        "    \n",
        "    unit_vector : bool, optional.\n",
        "        Whether to plot unit vectors of the standard basis, default to True.\n",
        "    \n",
        "    unit_circle: bool, optional.\n",
        "        Whether to plot unit circle, default to False.\n",
        "      \n",
        "    \"\"\"\n",
        "    nplots = len(matrices) + 1\n",
        "    nx = 2\n",
        "    ny = ceil(nplots/nx)\n",
        "    with plt.rc_context({\"figure.dpi\": 200, 'font.family':'serif', 'axes.axisbelow':True, 'font.size':fontsize, \"axes.titlesize\":5, \"lines.linewidth\":1}):\n",
        "      figsize = numpy.array([2*nx, 2*ny]) * fig_scale\n",
        "      figure, axes = pyplot.subplots(nx, ny, figsize=figsize)\n",
        "\n",
        "      for i in range(nplots):  # fig_idx \n",
        "          if i == 0:\n",
        "              matrix_trans = numpy.identity(2)\n",
        "              title = 'Before transformation'\n",
        "          else:\n",
        "              matrix_trans = matrices[i-1] @ matrix_trans\n",
        "              if i == 1:\n",
        "                  title = 'After {} transformation'.format(i)\n",
        "              else:\n",
        "                  title = 'After {} transformations'.format(i)\n",
        "          plot_transformation_helper(axes[i//nx, i%nx], matrix_trans, unit_vector=unit_vector, unit_circle=unit_circle, title=title)\n",
        "      # hide axes of the extra subplot (only when nplots is an odd number)\n",
        "      if nx*ny > nplots:\n",
        "          axes[-1,-1].axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQ4WLMMb4Rb_",
        "cellView": "form"
      },
      "source": [
        "# @markdown Helper functions\n",
        "def sort_evals_descending(evals, evectors):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order. Also aligns first two\n",
        "  eigenvectors to be in first two quadrants (if 2D).\n",
        "\n",
        "  Args:\n",
        "    evals (numpy array of floats)    : Vector of eigenvalues\n",
        "    evectors (numpy array of floats) : Corresponding matrix of eigenvectors\n",
        "                                        each column corresponds to a different\n",
        "                                        eigenvalue\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)          : Vector of eigenvalues after sorting\n",
        "    (numpy array of floats)          : Matrix of eigenvectors after sorting\n",
        "  \"\"\"\n",
        "\n",
        "  index = np.flip(np.argsort(evals))\n",
        "  evals = evals[index]\n",
        "  evectors = evectors[:, index]\n",
        "  if evals.shape[0] == 2:\n",
        "    if np.arccos(np.matmul(evectors[:, 0],\n",
        "                           1 / np.sqrt(2) * np.array([1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 0] = -evectors[:, 0]\n",
        "    if np.arccos(np.matmul(evectors[:, 1],\n",
        "                           1 / np.sqrt(2) * np.array([-1, 1]))) > np.pi / 2:\n",
        "      evectors[:, 1] = -evectors[:, 1]\n",
        "  return evals, evectors\n",
        "\n",
        "def calculate_cov_matrix(var_1, var_2, corr_coef):\n",
        "  \"\"\"\n",
        "  Calculates the covariance matrix based on the variances and\n",
        "  correlation coefficient.\n",
        "\n",
        "  Args:\n",
        "    var_1 (scalar)         :  variance of the first random variable\n",
        "    var_2 (scalar)         :  variance of the second random variable\n",
        "    corr_coef (scalar)     :  correlation coefficient\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats) : covariance matrix\n",
        "  \"\"\"\n",
        "  cov = corr_coef * np.sqrt(var_1 * var_2)\n",
        "  cov_matrix = np.array([[var_1, cov], [cov, var_2]])\n",
        "  return cov_matrix\n",
        "\n",
        "def get_variance_explained(evals):\n",
        "  \"\"\"\n",
        "  Plots eigenvalues.\n",
        "\n",
        "  Args:\n",
        "    (numpy array of floats) : Vector of eigenvalues\n",
        "\n",
        "  Returns:\n",
        "    Nothing.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # cumulatively sum the eigenvalues\n",
        "  csum = np.cumsum(evals)\n",
        "  # normalize by the sum of eigenvalues\n",
        "  variance_explained = csum / np.sum(evals)\n",
        "\n",
        "  return variance_explained\n",
        "\n",
        "def add_noise(X, frac_noisy_pixels):\n",
        "  \"\"\"\n",
        "  Randomly corrupts a fraction of the pixels by setting them to random values.\n",
        "\n",
        "  Args:\n",
        "     X (numpy array of floats)  : Data matrix\n",
        "     frac_noisy_pixels (scalar) : Fraction of noisy pixels\n",
        "\n",
        "  Returns:\n",
        "     (numpy array of floats)    : Data matrix + noise\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  X_noisy = np.reshape(X, (X.shape[0] * X.shape[1]))\n",
        "  N_noise_ixs = int(X_noisy.shape[0] * frac_noisy_pixels)\n",
        "  noise_ixs = np.random.choice(X_noisy.shape[0], size=N_noise_ixs,\n",
        "                               replace=False)\n",
        "  X_noisy[noise_ixs] = np.random.uniform(0, 255, noise_ixs.shape)\n",
        "  X_noisy = np.reshape(X_noisy, (X.shape[0], X.shape[1]))\n",
        "\n",
        "  return X_noisy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtUJK2gYommf"
      },
      "source": [
        "# Key concept review & coding tips\n",
        "\n",
        "## Special matrices\n",
        "\n",
        "### Diagonal matrices\n",
        "*   Have only nonzero entries on the diagonal \n",
        "*   Can be rectangular\n",
        "*   Scales space\n",
        "*   Inverse is diagonal matrix with inverse entries\n",
        "\n",
        "### Orthogonal matrices\n",
        "*  Square matrix where every column is a unit vector and every pair of columns is orthogonal\n",
        "*  Rotates space\n",
        "*  Its inverse is its transpose\n",
        "\n",
        "### Symmetric matrices\n",
        "*  Square matrix where $a_{ij} = a_{ji}$\n",
        "*  Equals its own transpose\n",
        "*  Eigenvalues are always real (not complex)\n",
        "*  Eigenvectors associated with different eigenvalues are orthogonal\n",
        "\n",
        "## Matrix decomposition\n",
        "\n",
        "* Factorization of a matrix into a product of matrices \n",
        "* Product matrices might be more compact/ordered, could make computations easier, could shed light on matrix structure\n",
        "\n",
        "### Eigendecomposition\n",
        "*  $A = VDV^{-1}$ where V has eigenvectors as columns and D is diagonal matrix with eigenvalues on the diagonal\n",
        "*  Can only do this if A is square and if eigenvectors of A form a basis for space\n",
        "*  $A^n = VD^nV^{-1}$\n",
        "*  $A^{-1} = VD^{-1}V^{-1}$\n",
        "\n",
        "### Singular value decomposition\n",
        "*  `np.linalg.svd`\n",
        "*  $A = USV^T$ where U/V are orthogonal matrices and S is diagonal\n",
        "*  Can decompose any matrix this way\n",
        "*  Diagonal entries of S are singular values, columns of U are left singular values, columns of V are right singular values\n",
        "* Decomposes transformation that matrix enacts into a rotation, then a scaling, then a rotation\n",
        "* Columns of V associated with zero or non-existant singular values form an orthogonal basis for the nullspace of A\n",
        "* Columns of U associated with non-zero singular values form an orthogonal basis for the column space of A\n",
        "* rank(A) = number of non-zero singular values\n",
        "* SVD factorizes A into sum of outer products with decreasing influence, can use first K sums to form rank K approximation of A\n",
        "\n",
        "## Dimensionality Reduction\n",
        "* Transform data from high D to low D while keeping as much information as possible about the data (finding new representation for the data)\n",
        "* Can help with data visualization, noise reduction, data preprocessing for further analyses, and scientific findings, among other things\n",
        "\n",
        "### Principal components analysis\n",
        "* `sklearn.decomposition.pca` for PCA, see info at the end of this tutorial\n",
        "*  Find axes in space that maximize the variance of the data (and minimize the residuals) while being orthongal to each other. Project data onto these axes and keep first K components\n",
        "* Can think of PCA as a change of basis where the new basis vectors are the principal components\n",
        "* $U = XV$ where U is the transformed data (# data points x reduced dim), X is the data matrix (# data points x orig dim), and V is the components matrix (orig dim x reduced dim)\n",
        "* Always center your data first!\n",
        "* Can find principal components as the eigenvectors of the covariance matrix ($\\frac{1}{n}X^TX$), eigenvalues tell you the variance explained by that component (plot these to make a scree plot) \n",
        "* Could also use SVD to find PCA - the columns of V are the eigenvectors of the covariance matrix and the squared singular values over N are the eigenvalues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31yvHJrIX97I"
      },
      "source": [
        "# Exercise 1: Delving into SVD\n",
        "\n",
        "We'll explore SVD by hand in this problem to help solidify our understanding of how the matrices interact with each other. Let $$A = \\begin{bmatrix}\n",
        "2 & -4 \\\\\n",
        "3 & 1 \\\\\n",
        "\\end{bmatrix}, \\bar{v} = \\begin{bmatrix}\n",
        "2 \\\\\n",
        "1 \\\\\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "In the following plot, we'll see the transformation that this matrix enacts.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qpt1J626izD"
      },
      "source": [
        "A = np.array([[2, -4], [3, 1]])\n",
        "plot_linear_transformation(A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZRBWdvEdT9C"
      },
      "source": [
        "## A) Computing the SVD\n",
        "\n",
        "Use `np.linalg.svd` to get the SVD of the matrix A. Note that the outputs are not quite the U, S, V we've been discussing. This function outputs $V^T$ directly. Get S/V from the outputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ialA_16dqJM"
      },
      "source": [
        "U, s, VT = ...\n",
        "S = ...\n",
        "V = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDKYQdd0mFPK"
      },
      "source": [
        "## B) SVD step by step transformations\n",
        "\n",
        "Multiply out the operations of $V^T$, S, and U with vector $\\bar{v}$ one at a time. In other words, get $V^T\\bar{v}$, then $SV^T\\bar{v}$, then $USV^t\\bar{v}$. You do not need to do this by hand - use code - but make sure you understand the matrix vector multiplication! \n",
        "\n",
        "Make sure $USV^t\\bar{v}$ = $A\\bar{v}$.\n",
        "\n",
        "Execute the following cell to visualize the vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmEmxqSNmoB8"
      },
      "source": [
        "v = ...\n",
        "VTv = ...\n",
        "SVTv = ...\n",
        "USVTv = ...\n",
        "Av = ...\n",
        "print(USVTv)\n",
        "print(Av)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7qX2EbFyCCU",
        "cellView": "form"
      },
      "source": [
        "# @title\n",
        "# @markdown Execute to visualize vector transforms\n",
        "vec_names = [r'$\\bar{v}$', r'$SV^T\\bar{v}$', r'$V^T\\bar{v}$', r'A$\\bar{v}$']\n",
        "vecs = np.array([v, \n",
        "                 SVTv,\n",
        "                 VTv,\n",
        "                 USVTv])\n",
        "\n",
        "fig, axes = plt.subplots(1, 1)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "\n",
        "axes.set(xlim=[-8, 8], ylim=[-8, 8])\n",
        "axes.axis('Off')\n",
        "\n",
        "for i_vec, vec in enumerate(vecs):    \n",
        "  axes.arrow(0, 0, vec[0], vec[1], head_width=.2, facecolor=colors[i_vec], edgecolor=colors[i_vec], length_includes_head=True);\n",
        "  axes.annotate(vec_names[i_vec], xy=(vec[0]+np.sign(vec[0])*.15, vec[1]+np.sign(vec[1])*.15), color=colors[i_vec]);\n",
        "\n",
        "axes.plot([0, 0], [-8, 8], classic, alpha=.4);\n",
        "axes.plot([-8, 8], [0, 0], classic, alpha=.4);\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlhGZPgryRQ6"
      },
      "source": [
        "What transformation is happening to $\\bar{v}$ at each step?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9bd8B87ySbP"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCxubbvrOiL9"
      },
      "source": [
        "## C) Low rank approximation \n",
        "\n",
        "We'll explore successful low rank approximations of receptive fields in Tutorial 2 and this will make the concept much more intuitive - the goal of this problem is just to understand the computation involved and what a rank 1 approximation means.\n",
        "\n",
        "Calculate a rank 1 approximation of A by hand. Specifically, compute:\n",
        "\n",
        "$$\\text{Rank 1 approx } = B = s_1\\bar{u}_1\\bar{v}_1^T $$\n",
        "\n",
        "where $s_1$ is the first (highest) singular value and $\\bar{u}_1$ and $\\bar{v}_1$ are the corresponding columns of U and V. \n",
        "\n",
        "Show your work for the computation! You should round to 2 places after the decimal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSm4PsBtRs9a"
      },
      "source": [
        "**Your math answer** show your work!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dU3Cn7orR--b"
      },
      "source": [
        "Compare B to the original matrix A. What does a rank 1 approximation mean? What is the computation \"trying to do\"? What is happening with the columns/rows of B? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IUl6mHYSGQK"
      },
      "source": [
        "**Your text answer here**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVJdlGKwSIXy"
      },
      "source": [
        "Note that the rank 1 approximation here is not great because our matrix is not anywhere close to rank 1! We would fully recover our matrix with a rank 2 approximation - $ A = s_1\\bar{u}_1\\bar{v}_1^T + s_2\\bar{u}_2\\bar{v}_2^T$ - since A is 2 x 2 and has maximum rank of 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XOGsCNtyahv"
      },
      "source": [
        "## (Optional) Extra info: Orthogonal matrices can also reflect\n",
        "\n",
        "Execute the next cell to visualize the transformation at each step of SVD (by $V^T$, then $S$, then $U$). You will notice that it isn't simply rotation, then scaling, then a rotation. Both $V^T$ and $U$ enact a reflection in addition to a rotation. Orthogonal matrices can reflect in addition to rotating space. \n",
        "\n",
        "We could get an SVD without reflection if we hadn't ordered our columns by the size of the singular values.  If we switched the columns in U, S, and V, we would see just a rotation, then scaling, then another rotation (show below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVfwxAKppUK7",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to visualize transformations\n",
        "\n",
        "plot_linear_transformations(VT, S, U, unit_vector=True, unit_circle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P97m4A8ht2Cs",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to visualize transformations with permuted columns\n",
        "\n",
        "plot_linear_transformations(V[:, [1, 0]].T, np.diag(s[::-1]), U[:, [1, 0]], unit_vector=True, unit_circle=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfgPXNMizbkN"
      },
      "source": [
        "# Exercise 2: PCA implementation and correlation exploration\n",
        "\n",
        "### Modified from NMA W1D5 T2\n",
        "\n",
        "In this exercise, you will implement PCA, apply it to 2 dimensional data, and examine the effects of correlations between dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BQ3zOJeEBHR",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to generate some data (X)\n",
        "np.random.seed(123)\n",
        "variance_1 = 1\n",
        "variance_2 = 1\n",
        "corr_coef = 0.8\n",
        "cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
        "X =  np.random.multivariate_normal([5, 10], cov_matrix, size=1000)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:,0], X[:, 1], s=1, color='#63BA79');\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "\n",
        "ax.set(xlabel='Neuron 1 activity', ylabel='Neuron 2 activity', xlim=[-5, 15], ylim=[-5, 15]);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0lbntNzCONT7"
      },
      "source": [
        "## A) Interactive Demo: Identifying first principal component\n",
        "\n",
        "Let's take a subset of our data as shown below and mean subtract it. Play with the interactive demo. About which value of theta represents the first principal component? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx5hhzZHU18K",
        "cellView": "form"
      },
      "source": [
        "# @markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "def plot_potential_component(theta=180):\n",
        "  n_points = 30\n",
        "\n",
        "  mean_subtracted_X = X - np.mean(X, 0)\n",
        "\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.spines['right'].set_visible(False)\n",
        "  ax.spines['top'].set_visible(False)\n",
        "\n",
        "  ax.set(xlabel='Neuron 1 activity', ylabel='Neuron 2 activity', xlim=[-5, 5], ylim=[-5, 5]);\n",
        "\n",
        "  w = np.asarray([np.cos(theta*np.pi/180), np.sin(theta*np.pi/180)])[None, :];\n",
        "  z = mean_subtracted_X[:n_points, :] @ w.T @ w;\n",
        "  for i in range(n_points):\n",
        "    ax.plot([mean_subtracted_X[i,0], z[i,0]], [mean_subtracted_X[i,1], z[i,1]], 'r')\n",
        "\n",
        "  ax.plot(w[0, 0]*5*np.array([-1, 1]), w[0, 1]*5*np.array([-1, 1]), 'k')\n",
        "  ax.scatter(z[:,0], z[:, 1],  color='r')\n",
        "  ax.scatter(mean_subtracted_X[:n_points,0], mean_subtracted_X[:n_points, 1], color='#63BA79');\n",
        "\n",
        "\n",
        "_ = widgets.interact(plot_potential_component, theta = (0, 180, 1), fontsize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr3lYuXhU0hM"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbG3UzoAz8PG"
      },
      "source": [
        "## B) Implement PCA \n",
        "\n",
        "Let's first implement PCA! We will build a function that takes in data and returns the transformed data, the principal components, and the variance explained by each component.\n",
        "\n",
        "We will use an implementation involving the eigenvectors/eigenvalues of the covariance matrix (as opposed to using SVD).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7CDch1fvIqY"
      },
      "source": [
        "def pca(X):\n",
        "  \"\"\"\n",
        "  Sorts eigenvalues and eigenvectors in decreasing order.\n",
        "\n",
        "  Args:\n",
        "    X (numpy array of floats): Data matrix each column corresponds to a\n",
        "                               different random variable \n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)  : Data projected onto the new basis\n",
        "    (numpy array of floats)  : Vector of eigenvalues\n",
        "    (numpy array of floats)  : Corresponding matrix of eigenvectors\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # Subtract the mean of X\n",
        "  X = ...\n",
        "\n",
        "  # Calculate the sample covariance matrix\n",
        "  cov_matrix = ... # hint: covariance matrix = (1/n)X^TX\n",
        "\n",
        "  # Calculate the eigenvalues and eigenvectors\n",
        "  evals, evectors = ... # hint: use np.linalg.eig\n",
        "\n",
        "  # Sort the eigenvalues in descending order using a helper function\n",
        "  evals, evectors = sort_evals_descending(evals, evectors)\n",
        "\n",
        "  # Project the data onto the new eigenvector basis\n",
        "  transformed_data = ... # hint: remember U = XV\n",
        "  \n",
        "  return transformed_data, evectors, evals\n",
        "\n",
        "# Uncomment below once you have filled in the above function\n",
        "\n",
        "# Perform PCA on the data matrix X\n",
        "#X_pca, evectors, evals = pca(X)\n",
        "\n",
        "# Plot the data projected into the new basis\n",
        "#plot_pca_transformation(X, X_pca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ca3ClyTb6F1h"
      },
      "source": [
        "Note that the correlation between dimensions goes to 0 after the transformation to the principal components basis! This is a property of PCA: it decorrelates the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEassDaK6_Ii"
      },
      "source": [
        "## C) Visualize variance explained\n",
        "\n",
        "We want to create a plot telling us the percent of variance explained by each principal component (here we have just two). Determine what information you need for this (the inputs) and complete the function below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzACc3Z8JXqj"
      },
      "source": [
        "def plot_variances(...):\n",
        "\n",
        "   percent_explained_variance = ...\n",
        "\n",
        "   fig, ax = plt.subplots() \n",
        "   colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "   ax.plot(percent_explained_variance, '-o', color=colors[4])\n",
        "   ax.set(ylim=[0, 1], ylabel='% Explained Variance', xlabel='Component number', xticks=np.arange(len(percent_explained_variance)))\n",
        "   \n",
        "plot_variances(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOlAMstb7PEJ"
      },
      "source": [
        "## D) Interactive Demo: Exploration of the correlation coefficient\n",
        "\n",
        "Run the following cell and use the slider to change the correlation coefficient of the data. This will update a plot of the data with the principal components overlaid and a plot of percentage of explained variance.\n",
        "\n",
        "**Questions:**\n",
        "* Can you find a correlation coefficient value for which the components have equal explained variance?\n",
        "* Can you find a value for which only only one component explains any variance?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJw30vF6Jizw",
        "cellView": "form"
      },
      "source": [
        "# @markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "def refresh(corr_coef=.8):\n",
        "  cov_matrix = calculate_cov_matrix(variance_1, variance_2, corr_coef)\n",
        "  X = X =  np.random.multivariate_normal([0, 0], cov_matrix, size=1000)\n",
        "  score, evectors, evals = pca(X)\n",
        "  plot_data_and_PCs(X, evectors)\n",
        "  plot_variances(evals)\n",
        "\n",
        "_ = widgets.interact(refresh, corr_coef=(-1, 1, .1), fontsize=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8dhaIewLT_-"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVl9D6vv-qPX"
      },
      "source": [
        "## Optional advanced challenge: PCA implementation with SVD\n",
        "\n",
        "Take the PCA function from part A and implement with SVD instead of with the eigenvectors of the covariance matrix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XN7xqpZ4OWX"
      },
      "source": [
        "def pca_with_SVD(X):\n",
        "\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SSIoA4M7V7n"
      },
      "source": [
        "# Exercise 3: PCA of images \n",
        "## Modified from NMA W1D5 T3\n",
        "\n",
        "In this exercise, we will look at the PCA of images. We will use images from the MNIST dataset, which is a dataset of handdrawn numbers (0-9). We're using this data instead of more neuroscience related data because it's a small dataset that's easy to interpret. Everything we will learn here could be applied to, for example, the frames of a video of a mouse performing a task. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l7sQ0ZGdoxP"
      },
      "source": [
        "The MNIST dataset consists of a 70,000 images of individual handwritten digits. Each image is a 28x28 pixel grayscale image. For convenience, each 28x28 pixel image is often unravelled into a single 784 (=28*28) element vector (a process called flattening the images), so that the whole dataset is represented as a 70,000 x 784 matrix. Each row represents a different image, and each column represents a different pixel.\n",
        " \n",
        "Execute the following cell to load the MNIST dataset and plot the first nine images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSMEbdlx7cdK"
      },
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml(name='mnist_784')\n",
        "X = mnist.data\n",
        "plot_sample_images(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04DQStqmeRpe"
      },
      "source": [
        "## A) Explained variances\n",
        "\n",
        "We will first perform PCA and plot the cumulative percentage explained variance over components. Note that this is related to our earlier plots but now we are plotting the percentage of explained variance **cumulatively**. Execute the next cell to do this. \n",
        "\n",
        "- How many principal components are required to explain 90% of the variance?\n",
        "- How does the intrinsic dimensionality of this dataset compare to its extrinsic dimensionality? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTF-oiCveGsv"
      },
      "source": [
        "transformed_data, evectors, evals = pca(X)\n",
        "variance_explained = get_variance_explained(evals)\n",
        "plot_variance_explained(variance_explained)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlnK9dsYeni5"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC0vE4ITflA4"
      },
      "source": [
        "## B) PCA Reconstruction\n",
        "\n",
        "Let's try projecting down onto our reduced dimensionality PCA space and then **reconstructing** our images from the low-D space.\n",
        "\n",
        "To see this, recall that to perform PCA we projected the data $\\bf X$ onto the eigenvectors of the covariance matrix:\n",
        "\\begin{equation}\n",
        "\\bf U = X V\n",
        "\\end{equation}\n",
        "where $U$ is the transformed data, $X$ is the data matrix, and $V$ is the components matrix.\n",
        "\n",
        "Since $\\bf V$ is an orthogonal matrix, ${\\bf V}^{-1} = {\\bf V}^T$. We can reconstruct by:\n",
        " \n",
        "\\begin{equation}\n",
        "\\bf X = U V^T\n",
        "\\end{equation}\n",
        "\n",
        "To reconstruct the data from a low-dimensional approximation, we just have to truncate these matrices.  Let's call ${\\bf U}_{1:K}$ and ${\\bf V}_{1:K}$ as keeping only the first $K$ columns of this matrix. Then our reconstruction is:\n",
        "\\begin{equation}\n",
        "{\\bf \\hat X = U}_{1:K} ({\\bf V}_{1:K})^T.\n",
        "\\end{equation}\n",
        "\n",
        "Complete the following function to reconstruct the images from the top K components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42Yta-O8eoba"
      },
      "source": [
        "def reconstruct_data(transformed_data, evectors, X_mean, K):\n",
        "  \"\"\"\n",
        "  Reconstruct the data based on the top K components.\n",
        "\n",
        "  Args:\n",
        "    transformed_data (numpy array of floats)    : data projected onto PCA basis\n",
        "    evectors (numpy array of floats) : Matrix of eigenvectors\n",
        "    X_mean (numpy array of floats)   : Vector corresponding to data mean\n",
        "    K (scalar)                       : Number of components to include\n",
        "\n",
        "  Returns:\n",
        "    (numpy array of floats)          : Matrix of reconstructed data\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  #################################################\n",
        "  ## TO DO for students: Reconstruct the original data in X_reconstructed\n",
        "  # Comment once you've filled in the function\n",
        "  raise NotImplementedError(\"Student exercise: reconstructing data function!\")\n",
        "  #################################################\n",
        "\n",
        "  # Reconstruct the data from the score and eigenvectors\n",
        "  # Don't forget to add the mean!!\n",
        "  X_reconstructed =  ...\n",
        "\n",
        "  return X_reconstructed\n",
        "\n",
        "\n",
        "K = 100\n",
        "\n",
        "# Fill in below then uncomment the last line\n",
        "\n",
        "# Reconstruct the data based on all components\n",
        "X_mean = ...\n",
        "X_reconstructed = ...\n",
        "\n",
        "# Plot the data and reconstruction\n",
        "# plot_reconstructions(X, X_reconstructed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzQ_pWJMis55"
      },
      "source": [
        "## C) Interactive Demo: Reconstruct the data matrix using different numbers of PCs\n",
        "\n",
        "Now run the code below and experiment with the slider to reconstruct the data matrix using different numbers of principal components.\n",
        "\n",
        "\n",
        "-  How many principal components are necessary to reconstruct the numbers (by eye)? How does this relate to the intrinsic dimensionality of the data?\n",
        "- Do you see any information in the data with only a single principal component?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haWPdnj3ixWF",
        "cellView": "form"
      },
      "source": [
        "# @markdown Make sure you execute this cell to enable the widget!\n",
        "\n",
        "\n",
        "def refresh(K=100):\n",
        "  X_reconstructed = reconstruct_data(transformed_data, evectors, X_mean, K)\n",
        "  plot_reconstructions(X, X_reconstructed)\n",
        "  plt.title('Reconstructed, K={}'.format(K))\n",
        "\n",
        "\n",
        "_ = widgets.interact(refresh, K=(1, 784, 10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkG78Dn2i7qN"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3qjQEQGjI0I"
      },
      "source": [
        "## D) Visualization of the principal components\n",
        "\n",
        "We can visualize the principal components as images by reversing the flattening. Here we plot using a differenet colormap than black & white as it highlights more structure.\n",
        "\n",
        "* What structure do you see in the first principal component? What kinds of images would this basis vector differentiate?\n",
        "* Try visualizing the second and third basis vectors. Do you see any structure? What about the 100th basis vector? 500th? 700th?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWx9GWIrjuBI"
      },
      "source": [
        "  plot_principal_components(evectors[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGh5PexVkeb2"
      },
      "source": [
        "**Your text answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt6HzB3UjLse"
      },
      "source": [
        "## (Read only) E) Denoising with PCA\n",
        "\n",
        "We will add some noise to our images to see how PCA reconstructions can be helpful for reducing noise. In this case, we will set 20% of the pixels to random values. We will then visualize some of the noisy images and the resulting cumulative variance explained plot.\n",
        "\n",
        "In the next cell, we will project the images onto the original PCA space (from the clean, not noisy, data) and then reconstruct from the top 50 components. Observe that this removes the noise quite effectively!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJZuksdGkux6",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute this cell to visualize noisy data\n",
        "np.random.seed(2020)  # set random seed\n",
        "X_noisy = add_noise(X, .2)\n",
        "score_noisy, evectors_noisy, evals_noisy = pca(X_noisy)\n",
        "variance_explained_noisy = get_variance_explained(evals_noisy)\n",
        "\n",
        "\n",
        "plot_sample_images(X_noisy)\n",
        "plot_variance_explained(variance_explained_noisy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx08LiVElRzx",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute to visualize denoised reconstructions\n",
        "X_noisy_mean = np.mean(X_noisy, 0)\n",
        "projX_noisy = np.matmul(X_noisy - X_noisy_mean, evectors)\n",
        "X_reconstructed = reconstruct_data(projX_noisy, evectors, X_noisy_mean, 50)\n",
        "\n",
        "plot_reconstructions(X_noisy, X_reconstructed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpZixEh3f6XL"
      },
      "source": [
        "# Extra info: PCA & Sklearn\n",
        "\n",
        "In this tutorial, we created our own functions to compute PCA and reconstruct images so we could better understand the algorithms. Usually though, you would  use `sklearn.decomposition.pca` to perform PCA. Sklearn is a class based package - I have a video explaining the basics of class based programming (object oriented programming) and a video on sklearn as part of my Pandemic Python for Neuroscientists course so check that out if interested. \n",
        "I'll demonstrate the basics here using some data with 3 features (X). \n",
        "\n",
        "See docs here: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKJq25k-y4dC",
        "cellView": "form"
      },
      "source": [
        "#  @markdown Execute to generate some data X\n",
        "np.random.seed(123)\n",
        "variance_1 = 1\n",
        "variance_2 = 1\n",
        "corr_coef = 0.8\n",
        "cov_matrix = np.array([[1, .2, .3], [.2, 1, .3], [.3, .3, 1]])\n",
        "X =  np.random.multivariate_normal([5, 10, 2], cov_matrix, size=1000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbmbY_cJzSKd"
      },
      "source": [
        "# Import PCA class\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Set up model, tell it the number of components you'll want to keep (if not set, all components will be kept)\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "# Fit the model to your data, aka find the principal components\n",
        "pca.fit(X)\n",
        "\n",
        "# Now you can access the principal components\n",
        "print(pca.components_)\n",
        "\n",
        "# And the % of explained variance for each component\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# You can transform your data now\n",
        "transformed_data = pca.transform(X)\n",
        "\n",
        "# You could have fit and transformed at the same time if you had wanted\n",
        "transformed_data = pca.fit_transform(X)\n",
        "\n",
        "# You can also reconstruct into the original space\n",
        "reconstruction = pca.inverse_transform(transformed_data)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}